All readings taken by the co-located sensor were measured in 30 second intervals, and timestamped using the on-board Real Time Clock before being saved to an SD card.  Since this was all done offline, no corrections for the timestamps could be applied.

In general, we found a 0.15-0.25\% drift in the RTC timestamps.  This results in 60-200 samples of error every three weeks or so.  To appropriately align these drifted, ~30 second values with our minute-resolved reference values, we applied two stages of correction.  To begin, we corrected the timestamps to reflect their actual time (assuming a constant delay between each, and knowing the precise time the measurements started and ended).  We then linearly interpolated between these corrected timestamp values to find on-the-minute values for every minute.

For the hour long samples of BAM data, we had to consolidate our minute data down to hourly values.  To match the process of the BAM sensor (which collects samples for the first 50 minutes of every hour, and measures for the last ten), we wrote a script to average every value in our machine learning arrays over the course of the first 50 minute of every hour, and throw away the last 10.

This is only the first step of pre-processing the data, however.  For our air quality signals, a calibration step is necessary before comparing the data and characterizing the sensor.  For the SmartCitizen CO and NO2 sensors, as well as for the Sharp particle sensor, their output is an uncalibrated mV value.  In the case of the Sharp sensor, the reading is inverted from the actual particle level (as it is a measure of light that makes it through the sensor without scattering).  These sensors were calibrated using a LMSE approach to optimize a simple scale factor or a scale factor and an offset.  The minimization function was run (1) on all of the data, (2) with the outliers (> 1 stdev) removed, (3) on long-term averages of the data, and (3) to only optimize values that were within some tolerance (throwing out sections that appeared to show disagreement and instead favoring a tighter fit on aligned data).  The most realistic, quality fit was chosen from amongst these options as the `calibrated' reference.

For the AlphaSense sensors, calibration was even more complicated.  These sensors come with a calibration sheet giving appropriate values.  The formula for most of their sensors is: {\small \[ppb = \frac{(we - we_{zero}) - (n*ae - ae_{zero})}{sensitivity}\]} where $we$ is the working electrode measurement in mV, $ae$ is the auxiliary electrode measurement in mV, $we_{zero}$ is the working electrode offset value in mV, $ae_{zero}$ is the auxiliary electrode offset value in mV, $n$ is a temperature dependent and sensor chemistry dependent scale factor, and the sensitivity is the mV/ppb gain factor of the instrumentation amplifier on the conditioning board.  For the cross-sensitive O3+NO2 reading, we use the calibrated NO2 values and subtract the resulting mV offset given the calibrated NO2 sensitivty: {\small \[ppb = \frac{(we - we_{zero}) - (n*ae - ae_{zero}) - \frac{no2_{ppb}}{x\_sensitivity\_to\_no2}}{sensitivity}\]} While we tried the provided calibration data-- as well as simple LMSE scaling-- we found the best agreement came from LMSE minimization using a bounded search of $we_{zero},  ae_{zero},$ and $sensitivity$.  In these cases, the seed values were the provided calibration terms from AlphaSense.  This type of calibration gave very strong results compared to other methods.

With calibrated data, an appropriate tolerance was chosen for each sensor value (typically $\pm$2-5\% of the full range, though a larger tolerance was chosen for particulate), and each reading was classified as `correct' if falling within that tolerance of the MassDEP reference measurement, or `incorrect' if falling outside of it. 

This data is now ready for machine learning using the logistic regression discussed in the introduction to this chapter.  We performed all of this analysis using python's scikit-learn machine learning toolbox-- however, some experimentation was done with the java-based (GUI driven) `Weka' toolkit (using a python ARFF file conversion library), as well as initial exploration with google's new tensor-flow library (which has logistic regression support, but which really shines for its deep learning ability and large dataset handling).  These additions tools, and additional techniques, will be explored in more depth in the near future.

The general outline of the machine learning process we applied to the data is as follows: (1) load in the feature values (approximately 150 of them) to predict our binary error classification, (2) impute (or fill in) missing values, (3) split our data into training and test sets used 5-fold cross-validation, (4) run a grid search over logistic regression parameter-space to find the best regularization coefficient and penalty terms, (5) train our new 'best model' using the five training sets, and (6) verify the results on the five test sets.  Importantly, two types of cross-validation are used-- a shuffled type and a chunked type.  In one case, data from the entire two month period is randomly selected to constitute training and test sets; in the other, the first several weeks are used to predict the last, the last several are used to predict the first, etc.  The difference in these results gives us important insight into algorithm robustness and the effect of seasonal variation on our predictions.  

Additionally, we use randomized decision trees to rank the importance of our features, as well as seven other feature reduction techniques (correlation, linear regression, random forest, lasso, RFE, ridge, and stability).  A reduced set of the top 15 features is then used to retrain our original Logistic Regression, and the results are compared.  The strength of agreement between feature reduction techniques can suggest meaningful predictive relationships, and the relative strength of the classifier with this reduced feature set can also corroborate strong causality for the top features.

There are two main metrics we use to evaluate our system performance.  The most obvious metric is to compare the right answer ('is the sensor actually in error?') with the predicted one ('do we think the sensor is in error?') and display our results in a 2x2 confusion matrix.  We can easily calculate the error rate of our algorithm from this matrix.

Logistic Regression offers a probability along with its prediction, however, so to fully evaluate the strength of our results we must take these probabilities into account.  Our second evaluation metric-- and the accepted standard for this type of evaluation-- is a Receiver Operating Characteristic (ROC) curve.  This curve plots the true positive rate (the number of correct predictions that a measurement is in error, normalized by the total number of errored air quality readings) against the false positive rate (the number of incorrect predictions that a measurement is in error normalized by the total number of accurate air quality readings).  We can compute a point on this graph by choosing an arbitrary threshold for our probability, and classifying every measurement as a predicting an error in our reading or not based on whether the probability that it is falls above or below this threshold value.  If we set our probability threshold at 50\%, we find the point corresponding to our original confusion matrix.

When we plot the points for every threshold value (from 0-100\%), we generate an ROC curve.  These curves start at (0,0) and end at (1,1) on our plot.  Random guessing will form a line between the points at a 45 degree angle.  Perfect accuracy with 100\% confidence will form a right triangle-- jumping immediately to a value of (0,1) on our graph before continuing horizontally over and meeting up with the upper right corner.  Real, meaningful predictions will likely fall somewhere in between.  The area under the ROC curve (AUC-ROC) is normalized to a value between 0 and 1, and frequently used to characterize the quality of predictions generated by our logistic regression in a more comprehensive way than a simple confusion matrix.  Generally speaking, values above 0.8 suggest our model has good-to-excellent predictive power as the number grows closer to one, and values above 0.7 represent reasonable predictions.  Values below this mark are marginal, with anything close to 0.5 suggesting total failure.

Once we've found the AUC-ROC for our optimized logistic regression, we compare the results for the 5-fold validation sets (one having been `shuffled' or randomized, and the other having been `chunked').  The shuffled version assumes no time dependent phenomena-- using randomly chosen samples throughout the entire test to predict random other samples interspersed throughout the test.  The chunked version is a more realistic model-- using data we've already gathered from one period of time to predict future data.  

We must be careful with the shuffled version-- errors frequently occur together in time (a sensor will misread for an hour or two in a row, giving a few hundred errors at once, likely because of underlying phenomena).  Monotonically increasing functions (like temperature) could serve as a proxy for time, and the algorithm might take advantage of this co-occurance to `predict' our error.  This is a classic example of overfit, and it should be simply to look at the underlying feature and error distributions to ensure that our model hasn't led us to incorrect conclusions.  This could lead to artificially strong results in the shuffled case.  If we see a strong predictive relationship for one of these variables in the shuffled case, it is important to make sure that the variable is not simply monotonic and co-occuring with one large window of consecutive poor readings.  

After verifying the quality of our shuffled results, we can compare the shuffled and blocked versions of the algorithm.  If the shuffled version still does substantially better than the block-wise version, it suggests that we haven't trained on enough data to capture season-agnostic predictive information.  However, when the two agree, it forms a powerful indication that (1) we've captured enough data to train across seasons, and (2) we have hit upon real and useful phenomena.  