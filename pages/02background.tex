\chapter{2. Background and Motivation}

\begin{quote}
\textit{If I had an hour to solve a problem and my life depended on it, I'd spend the first 55 minutes determining the proper question to ask. \newline\begin{flushright}- Albert Einstein\end{flushright}} \newline
\end{quote}
\newline

There are many issues standing in the way of low-cost, portable, and effective air quality measurement.  While the field is ripe for innovation, a thorough understanding of the state-of-the-art (and its limiting factors) is important before attempting to make progress.  The following section is a distillation of advice from industry experts and thought leaders, who greatly informed the direction of this work.  

\section{Air Monitoring}

\subsection{Air Pollutants}

Despite the complexities behind sparse air quality measurement and individual health, there is a plethora of evidence linking small particulate matter and other pollutants to serious negative health effects. \cite{who2014, who2014_2, filley1954, allred1989, schwartz2005}

Particulate matter is designated according to its size-- PM2.5 are particles with 2.5 micron diameter or less, PM10 has a 10 micron diameter or less, and ultrafine particulate (UFP) are measured in nanometers (equivalent to PM1).  These designations are made based on human physiology.  Particles larger than 10 microns are typically filtered in the nose and throat; below this size, the particles are considered 'respirable'.  Particles between 10 and 2.5 microns can usually penetrate into the lungs and settle, while particles less than 2.5 microns tend to pass into the alveoli and into the bloodstream.  Nanoparticles are so small that some can pass through cell membranes, and damage other organs throughout the body.  Additionally, particulate deposition is different for these groups-- PM10 may settle out of the air in hours, while the smaller and lighter particles generally stay in the air until they are washed out with precipitation.

Particulate size distribution is generally viewed as the sum of \textit{n} log-normal distributions.  \cite{molenar2005} Nucleation by-products from engine combustion drives a positively-skewed, log-normal distribution centered at a few hundred nanometers.  \cite{particulate} Mechanically generated road dust on paved and unpaved roads generates a negatively-skewed log-normal particle distributions favoring 10 micron diameters.  Pollen also has a negatively skewed log-normal distribution in the 10-100 micron range. \cite{particulate} The greatest health risk is associated with the smallest diameter (often nucleation-based) particulate.  

Specific gases have also been linked to health risk, and the United States Environmental Protection Agency (EPA) has set standards for five other pollutants-- Lead, Nitrogen Oxides, Carbon Monoxide, Sulphur Dioxide, and Ozone. \cite{epa2014}

The main sources of lead exposure (automotive fuel and paint) have been heavily regulated over the past several decades in first world countries.  The last industrial lead smelter in the United States shut down two years ago, and airborne lead exposure in the first world has largely been eliminated other than from old house paint (which can aerosolize as houses are retrofitted or torn down.)  Airborne lead is still an issue in developing countries, however, especially near unregulated smelters that recycle car batteries.

The result of incomplete or high temperature automotive combustion, Nitrogen Oxides and Carbon Monoxide (as well as Black Carbon, a major constituent of PM2.5) are common pollutants in the urban setting.  Since the sources of these pollutants are mobile, they often manifest with complex spatiotemporal dynamics, including temporary hotspots and/or highly localized areas of high exposure.

Sulfur Dioxide is the result of fossil fuel combustion, largely coal and heavy oil, and is thus detectable near power plants and other industrial operations. 
  
Ozone at the earth's surface is typically the result of Volatile Organic Compounds (VOCs) reacting with Nitrogen Oxides in the presence of sunlight.  Thus, while vehicle emissions seed the process, ozone concentrations tend to be dependent on sunlight, and therefore more predictable and less dynamic than CO, NO2, and PM.

EPA standards are set at 75 parts per billion (ppb) average per hour for SO2, 100 ppb average per hour for NO2, 70 ppb average for 8 hours for Ozone, 9 parts per million (ppm) average for 8 hours for CO, and annual averages of 0.15 \(\mu g/m^3\) for Pb, 12 \(\mu g/m^3\) for PM2.5, and 35 \(\mu g/m^3\) for PM10. \cite{epa2014}  In the U.S. air pollutants have declined by over 60\% since 1990, however 121 million people still live in counties were these standards are not met. \cite{epa2014}  In the developing world these standards are rarely met in urban environments. 

\subsection{Sensor Technologies for Particulate Matter}

\begin{marginfigure}[3.5cm]
 	\includegraphics[width=\textwidth]{visuals/as_optical}               
 	 \caption{AlphaSense Optical PM2.5 Sensors}
  	\label{fig:as_optical}
\end{marginfigure}

There are many sensor modalities for pollution monitoring, with a handful considered reference grade.  For measuring particulate matter, high quality installations will deploy either a Beta Attenuation Monitor (BAM) or a Tapered Element Oscillating Microbalance (TEOM) sensor.  BAM sensors collect particulate on successive circular sections of a long filter that is spooled inside the device.  Measurements are taken by simply analyzing the beta particle attenuation through the filter.  TEOM sensors draw air through a filter so particulate deposits on the end of an oscillating cantilever (its resonant frequency is dependent on its mass).  The more mass that is deposited on the filter, the lower the resonant frequency, which is measured and used to calculate very accurate particulate levels.  Other methods typically include gravimetric techniques with filters that are analyzed in a lab environment. \cite{dep1999}

Optical methods for particulate sensing are particularly important in the mobile context due to their size and robustness.  Since particulates are measured in \(\mu g/m^3\), their accuracy depends on assumptions about airflow through the device, as well as the assumed statistics of particle size and mass distributions (which can change from location to location depending on the local sources and mixtures of pollutants).  

Optical PM sensors typically have similar geometry- a narrowly focused IR beam is broken by the particles, and the scattered light is measured by an off-angle photodiode.  For cheap sensors such as smoke alarms, airflow through the device is not tightly controlled (it may be driven by convection with a small heating element, but it is never consistent), the optics are coarse, and the captured light is only a loose representation of particulate level.  Better sensors use a more tightly focused beam, as well as a fan to control airflow, and can count pulses as the beam breaks-- thus the length of time the beam is broken is proportional to particle size.  The final and best type of optical technique uses Mie Scattering, which uses the intensity of scattered light on the photodiode to calculate particle size (a nonlinear monotonic function with particle size). \cite{morpurgo2012} Using this information (while observing the duration of a particle in front of the beam), it is possible to calculate flow rate and corrected for any inconsistencies between the designed and observed air speed.   Accurate optical variants include Condensation Particle Counters, in which particle size is increased in a predictable way by condensing vapor from a working fluid around the particles, making them easier to count. Electrical mobility sorting based on size/charge of particle in electrostatic field is also sometimes used in concert with optical techniques.

%In all of these designs, larger particles have a non-trivial deposition rate, which further constrains the geometry and flow through a device.  ... in designs without size selective filtration, larger particle desposition is influenced by gravity and wind, further constraining device geometry and orientation.

Inlets that protect these devices from wind, and thus help to create predictable airflow, are included on all professional-level equipment.  These inlets typically include particle size selectivity.  Most commonly, size selectivity is achieved using inertial techniques (i.e. impaction or cyclone filtering).  Inlets are generally mounted in an upright position due to particle deposition (so gravity works with the inlet), and are sometimes heated to evaporate fog.

For cheap, small, or mobile applications, optical sensing is the dominant modality.  On the high end (>\$15k), handheld systems like the Grimm Enviro 11E have sophisticated inlets for size selectivity, advanced optics, and create sheathing airflow out of filtered air, which helps collimate incoming samples and clean the beam path.  These professional-quality systems have been used in research studies to evaluate personal exposure, but they do not offer a viable option for distributed or personal applications.  

On the affordable end of the spectrum there are many sensors.  Unfortunately, independent tests have shown that none of these produce repeatable, accurate measurements in dynamic, real-world situations. \cite{williams2014} One in particular worth noting is the Shinyei PPD42NS, a \$10 sensor that is widely used and cited as providing high quality results. \cite{prabakar2015, austin2015} It has a small heating element for inducing convective flow, and coarse optics.  Unfortunately, its reputation is based on very controlled conditions.  In EPA-conducted outdoor tests, the Shinyei demonstrated cross-sensitivity to variations in airflow, temperature, and humidity. \cite{epa2014}  The lack of clarity around the application space in which cheap optical sensors can be effective is unfortunate, and highlights the importance of testing these sensors in realistic mobile contexts.

The \$300 Dylos seems to be the cheapest optical sensor with a strong linear relationship to Federal Reference Measurements (at <95\% relative humidity levels) in independent tests. \cite{williams2014} It uses an IR laser and has a much larger and more sophisticated flow design than its cheaper counterparts. \cite{compact2011}

The \$400 OPC-N2 is an optical particle sensor similar to the Dylos, but much smaller.  It uses a fan to drive a fixed flow rate, and take advantage of Mie Scattering principles to correct for variations in flow rate through the device.  AlphaSense, its manufacturer, has not released a full characterization of their design, but preliminary testing in environmental sensing groups at MIT and the South Coast Air Quality Management District (SCAQMD) have yielded mixed results-- it appears that the OPC-N2 is incapable of sensing particles below a few hundred nm in diameter, which make up most of the mass concentration of PM2.5.  \cite{personal2015, scaqmd2015}

Since PM2.5 mass is largely made up of nucleation/combustion driven particulate, its core component follows a log-normal size distribution centered in the nm range.  While measuring the log tail can provide some insight into the core of the distribution, tails from larger particulate like mechanically-created road dust or pollen (mostly 10-100micron) overlap in the critical 300nm-10micron range where the OPC-N2 is sensitive.  This presents serious challenges inferring a relationship between what is actually measured in the 300nm-10micron range and PM2.5 levels without extra information.  

\subsection{Sensor Technologies for Gas Sensing}

\begin{marginfigure}[3.5cm]
 	\includegraphics[width=\textwidth]{visuals/as_gas}               
 	 \caption{AlphaSense Electrochemical Gas Sensors}
  	\label{fig:as_gas}
\end{marginfigure}

For sensing specific gases, many types of sensors are used.  Among other techniques, spectroscopy, chromatography, and chemiluminescence are very common for professional applications.  For mobile use, Alphasense sensors have emerged with a strong cost to performance ratio.  Alphasense sells Photoionization Detection based sensors, which work by ionizing gas particles with UV light and sensing the generated current over a fixed voltage in contact with the air. They also sell Nondispersive IR sensors, a simple optical absorption method.  

For the specific types of gases we're interested in, electrochemical techniques are the primary low cost method on the market.  The AlphaSense version is well-regarded, with ppb sensitivities and a clear failure condition (instead of the gradual drift you might expect as the sensor is depleted and dirtied). Electrochemical gas sensors are comprised of a working electrode, a reference electrode, and a counter electrode, all bathed in an electrolyte.  The reference electrode is used to control the voltage at the working electrode, and keep it in a linear current/voltage regime.  The working and counter electrodes promote inverse oxidation/reduction reactions, combining with the gas to produce free electrons, and then balancing that first reaction so as not to deplete or change the available reactants. The resulting current is proportional to the gas concentration, as long as corrections are applied for temperature, humidity, and pressure (and adequate time is allowed for 'warming up' once the reference electrode is powered on due to large inter-electrode capacitance). \cite{alpha2014} These sensors are generally well-characterized under stable operating conditions, and have well understood cross-sensitivities and time-constants associated with their behavior.

While AlphaSense sensors can be purchased with calibration data, environmental sensing researchers at MIT have suggested that these calibrations are generally not accurate, and typically co-locate the sensors with a Federal Reference sensor for more rigorous calibration before deploying them elsewhere. \cite{personal2015}

\subsection{Measurement Strategies and Complications}

Historically, the standard measure of air quality has been a sparse network of fixed stations run by government agencies.  These expensive and large stations require careful manual calibration every few weeks.
	
While these stations provide accurate data, studies have shown they either chronically underreport or have no correlation with the personal exposure of the citizens living near them. \cite{steinle2013} Only with sophisticated modeling of elevation, geography, ambient conditions, wind velocity, and land use can these data be tied to exposure elsewhere in a city, and these models must be evaluated on a case-by-case and pollutant-by-pollutant basis.  
	
New techniques are emerging to map and model a city using a small number of medium quality, mobile sensors. \cite{hasen2014} Stationary, high quality sensors play an important role in calibrating these systems on-the-fly, but these methods have shown much better predictive power for mapping cities in higher spatial resolutions.  However, their predictive power is still best on timescales of years and weeks, and starts to break down as they move towards days and hours.

Models on these timescales and resolutions are useful for understanding general trends in exposure for a city, as well as identifying and eliminating pollution sources, hotspots of high exposure, and issues with urban planning.  However, even these mobile techniques for map generation are limited in their ability to predict personal exposure with high spatio-temporal resolution.

Personal exposure is so difficult to measure because pollutant concentrations can vary dynamically.  For certain conditions researchers have modeled this complex behavior, and thanks to expensive portable sensors there have been several studies to corroborate their findings.  One such dynamic system that has been analyzed extensively is the 'urban canyon'-- a street with two tall buildings on either side that creates several interacting, swirling vortices \cite{vard2003}

Measurements have shown CO and UFP concentrations doubling on one side of the street relative to the other in an urban canyon, measured at the same time of day. \cite{xie2003} This variability has been demonstrated time and again-- one study showed complex relationships between different pollutants measured in the center of the street versus the sidewalk. \cite{raw2014} Different corners of the same intersection can also vary tremendously. \cite{kaur2007} Even walking roadside vs. building-side on the same sidewalk has been linked to significant differences in pollution exposure level. \cite{batt2014}

This is all to say that spatial variation is extremely high in some situations.  Concentrations can change drastically over just a few meters.  For accurate personal exposure monitoring, best practice is to sample air within 30cm of the mouth and nose. \cite{steinle2013, adams2009} While some of these spatial phenomena may fit an urban canyon model, and some may be modeled accurately with standard dispersion models, pollutant levels in general are hard to predict with any single technique (especially to within a few meters). \cite{hoek2008}  Given the current state of air pollution modeling, it is extremely difficult to predict spatial variation at a scale relevant to personal health outcomes without direct measurements. \cite{steinle2013, adams2009}

Temporal variation is equally difficult to monitor. \cite{lai2004, delga2012} Studies at traffic intersections have shown that regular, tenfold increases in pollutant concentration can occur over one second intervals. \cite{goel2014, wang2008} This staggering variation is averaged out even with the some of the best 'real-time' techniques-- fifteen second integration could miss an entire elevated concentration event.  Peak exposure levels may have important health implications, and transient events may account for the majority of urban exposure. \cite{lai2004, delga2012, goel2014, wang2008}

Given the tremendous spatiotemporal variation in pollution, fine-grained and distributed sampling in the lived environment seems to be the only viable path to accurate personal exposure data.  In dynamic environments, personal and mobile sensors offer the most direct path to these data.  While a dense fixed sensor network could provide similar insights, it would require many, intelligently placed devices to recreate the spatiotemporal resolution required to match the exposure estimates of a few personal sensors.  Verifying the predictive radius of a fixed sensor in a complex, urban environment is also non-trivial.  Personal, mobile sensing bypasses these complications.

With enough adoption, portable sensing could improve the collection and prediction of city-wide pollution mapping traditionally associated with fixed sensor installations.  Eventually, distributed (and likely mobile) sensor data may even enable accurate path-based personal exposure modeling (statistically relevant on the order of meters and minutes), since the data is collected in the real microenvironments that dominate their exposure.  Accounting for this otherwise highly specific spatiotemporal resolution would be difficult with any alternative method (day-level and 100 \(m^2\) resolution is the best we see with predictive models right now).  As sensors increases in accuracy and drops in cost, distributed sensing will usher in a new way of understanding the pollution landscape and our exposure to it.


\section{Sensor Networks}

\subsection{Air Quality Sensor Networks}

It is not uncommon to see publications describing cheap and portable smart-phone based air quality projects. \cite{cheng2014, haze2013}  In most cases, these publications focus on system design, and produce thought-provoking work on the user-interface. \cite{dutta2009}  In cases where technologists explore new sensor design, it is rare they achieve compelling improvements.  The past 20 years has seen a lot of incremental optimization in the most promising sensing modalities.  Few research labs are positioned to push the state-of-the-art further by simply re-applying the same core physics without a new fundamental insight.  

Outside of phone applications, true system-level research in the air quality space is uncommon.  Most air quality networks use the same topology-- one type of sensor device with standard, centralized data collection methods.  The exception to this rule comes out of ETH Zurich's OpenSense project, where mobile sensors check their calibration as they pass higher-quality fixed sensors.  \cite{hasen2011}  OpenSense has also pioneered methods for multi-hop mobile sensor calibration.  Their work sets the standard for exploratory new air quality sensor network topologies.

In the consumer space, many projects and devices are being launched.  Unfortunately, most devices do not stand up to scrutiny, and rarely do they offer technical innovation.  None of these devices has succeeded at sustaining momentum with its adopters.  SmartCitizen is an example-- after a successful 2014 kickstarter with 600 backers and \$68k raised, the SmartCitizen online network currently shows no active devices (despite 618 having been registered). \cite{sck}  The constant barrage of 'new' monitoring devices-- without accountability, without rigorous data-collection, and without real-world use-cases-- saturates and dilutes consumer interest in these important issues.   

Citizens aren't the only ones purchasing air quality sensor devices.  Many cities are installing high-density pollution monitoring networks-- in some cases, only later realizing that the data is not of sufficient quality to be of any use.  London (quite publicly) recently released a network of GPS-tracked, tweeting pigeons with NO2 sensor backpacks-- while driven by a marketing firm as a (very successful) publicity campaign, no data has confirmed the value of this mediagenic approach. \cite{london2016}

The EPA publicly states that distributed, cheap sensing technology will be a cornerstone of their future success. \cite{khan2015}  As part of the effort to engage with active citizens and communities, the EPA measures and publishes data about low cost consumer devices. \cite{williams2014}  Currently this is done by co-locating the consumer device with a Federal Reference Measurement (FRM) device outside for several months (usually through a change of seasons).  This validation is not standardized or rigorously defined in length, season, analysis, or in number of devices tested.  Generally, the end result is a simple regression comparison (produced by hand), and a single designation for the sensor (i.e. 'good' or 'bad').  Other organizations do similar co-location experiments (like SCAQMD) in very different climates using different standards. \cite{scaqmd}

\subsection{Large Scale Data Sharing}

The air quality research community is actively looking for solutions to facilitate inter-organization data-sharing, so that large scale collaboration can become more commonplace.  They are also actively working to educate, involve, and benefit from the citizen sensing movement.  There are many open questions around how to structure an ecosystem with variable quality data, how to define data standards, and how data should be hosted. 
 
The most common solution for large-scale data sharing is to construct a centralized 'cloud' database with strict data standards and a strict ontology.  Generally, users prepare their data to meet the standard, and then push their data to the database using some basic tools.  As the most common structure, there are options that have library support for various file formats and hardware platforms.  Examples include data.sparkfun.com (which integrates directly with Arduino shields) or plenar.io (which has easy csv upload and flexible data selection/access features).  

\subsection{ChainAPI}

When it comes to robust solutions for large scale sensor networks that directly feed into a database, the possible options are less well-defined.  An ideal ecosystem would allow large scale networks to interact seamlessly, while still allowing freedom in ontology and distributed hosting (similar to the World Wide Web).  While there are several solutions starting to appear for Internet of Things applications (the so called 'Web of Things'), many are over-specified, and up until now the practical result has been for industry players to silo their hardware, their data, and their applications.  Industry consortia are starting to address these problems,  but the issues are currently unresolved.

ChainAPI \cite{chainGit, chainPaper} is a thin, HAL- and JSON-based hypermedia solution for creating distributed, browsable data resources.  It dictates enough structure to make resources easily linkable, new ontological relationships easily definable, and datasets easily accessible, searchable, and streamable.  It leaves open the questions of ontology and backend database structure.

ChainAPI has been successfully used for a large-scale sensor ecological installation in Southern Massachusetts. \cite{ tidmarsh}  It serves as the backbone for many interesting data visualizations, audio mappings, sensor browsers, and future-looking tools.  It provides extensible answers to many questions facing the world of large scale, distributed sensor installations and their associated data.

\subsection{Machine Learning}

Machine learning provides a way to design algorithms that learn and improve as more data is provided.  These techniques have been applied to sensor data in a variety of forms.  Examples range from predicting the number of people in a closed space by looking at changes in distributed sensor readings (i.e. temperature, humidity, light, and pressure), to predicting soil moisture based on remote sensing techniques (i.e. vegetation index and light backscatter). \cite{mor2010, ahmad2010}  One notable research project used HVAC sensor data, both analytically and redundantly, to predict and verify when a sensor in the network has failed and automatically replace its unreliable data. \cite{smith2013}

Generally, these examples use supervised learning approaches with some form of cross-validation to validate success.  While each uses a different core algorithm (and there is room to test and apply all of them to the air quality space), one worth mentioning is the logistic regression.  Logistic regression is frequently used to predict engineering failure of products or systems. \cite{logistic}  It can be applied as a binary classifier (i.e., 'Is this sensor failing?'), as well as give a probability for each outcome (75\% likey for 'yes' and  25\% likely for 'no').
  

\section{Motivation}

Air pollution poses a risk to the health of people around the world.  While standardized measurement techniques are highly accurate, they are also extremely expensive and historically only limited to a number of stationary sites.  These stationary sensors do not capture meaningful information about a citizen's personal exposure-- the spatiotemporal variations of pollution concentration are too complex and too narrowly resolved to be captured with a single, distant sensor.

For accurate personal monitoring, wearable, mobile sensing offers a very attractive approach.  Sensors do exist that can measure personal exposure, but there is a tight relationship between cost and accuracy-- most are cheap and inaccurate.  While elusive, portable and affordable sensing has the potential to offer powerful insights for both individuals and research organizations.

The lack of cheap solutions is not due to a lack of understanding.  The core device physics of most sensors have been well-optimized over several decades, and the sophistication underlying reference level equipment is truly remarkable.  Affordable sensors are starting to mirror the core principles of instrument-grade devices.  Unfortunately, there are systematic failures in low-cost systems that are fundamental to the underlying sensor modality.  

Optical sensors, for instance, require precision optics, heated inlets, flow control, and size-selective filtration.  Solutions to these problems require extra power, extra size, or extra cost (and frequently all three).  Addressing these problems likely results in a sensor that is neither cheap nor portable.  Electrochemical gas sensors require a clean, precision doping process, a statistically-defined minimal exposed surface area, and compensation for flow rate, pressure, temperature, humidity, and electrical noise.  The physics limit how small they can be, and the market limits how much cost can be driven out of the manufacturing process.

There are two main take-aways from this section-- the first is that attempting to incrementally improve devices by re-exploring their core physics is a difficult proposition.  The core physics are well-understood and companies have been optimizing them successfully for decades.  \textbf{The first order problems with cheap sensors are \textit{not} with the core device principles, but with well-understood failure modes (like flow control, fog, temperature dependence, or chemical cross-sensitivity).}

Since the core physics underlying cheap commercial sensors are approaching a very high quality,  we can assume that inaccuracy is likely the result of systemic, predictable failures.  If there was a single point of failure (like an optical sensor that is reliable except when fog is present), it would be trivial to predict when the sensor data is reliable based on fog measurements.  In real-world scenarios, however, multiple failure modes compound and obfuscate these underlying predictive patterns.

\textbf{If cheap sensors are entering a quality regime where failure is increasingly predictable, it leads us to machine learning as a potentially powerful mechanism to improve reliability.}  Machine learning is perfectly suited to tease out these complicated underlying relationships.  Instead of the common approach of \textit{improving} sensor performance, the research suggests that \textit{characterizing} and \textit{predicting} sensor reliability in a nuanced way is novel, necessary, and potentially revolutionary.

In many cases, first-order predictions may work well to predict sensor accuracy.  Gas sensors break in known ways and are specified for known operating ranges.  Simple monitoring of temperature/humidity/pressure exposure, its air-flow, and gas sensors of cross-sensitive pollutants could provide extremely useful insight.  

Second order insights are perhaps more interesting.  For instance, the OPC-N2 particle counter is likely to be confounded by road dust or pollen.  What if we could loosely approximate road dust exposure based on the user's location relative to a road, traffic patterns, the time of day, and the wind?  What if we could predict O3 measurement reliability based on the underlying drivers-- NO2, sunlight, and cloud cover? In this thesis, we explore both first-order and second-order insights using machine learning techniques.

Simple machine learning analysis could also provide an objective measure of sensor quality.  How well machine learning can predict a sensor's behavior is a nuanced way to measure its repeatability.  Sensors that fail randomly instead of predictably are inferior in design and construction.  
     
For any of this to work, we need to compare our low-cost sensor against a high quality reference, so we can learn when it is providing spurious data, and what conditions may be indicative of that error.  There is precedent for air quality network infrastructure that compares cheaper mobile sensors with a higher quality reference, but until now this has only been done as a basic calibration step.  What we are proposing is the first self-correcting air quality system. 

In order to build such a system, we require a backend solution that can automatically compare EPA data to a cheaper network installation.  ChainAPI is well suited for this task, and in the process of building this infrastructure, we examine and address some of the biggest issues facing air quality data sharing, ecosystem building, and data interaction.  We also create an infrastructure that may be used to automatically measure and characterize consumer device quality, in a very nuanced, climate- and geography- specific way.

Finally, we believe such a system has the ability to contribute to and provoke a more nuanced, informed dialog in the citizen sensing community.  We propose the first device designed with the assumption that its data \textit{won't be consistently reliable}, that we need to predict when it is useful and when it is not.  Inherent in the design is the suggestion that the a sensor's success is complex, based on a variety of factors.  This provocation could help inform and educate new users-- cutting through noise instead of adding to it.

There are many interesting problems currently facing the air quality community.  We believe a machine learning approach to predicting sensor accuracy could improve the reliability of cheap sensors, pushing the state-of-the-art forward.   Validating data from affordable sensors would opening up a world of reliable, distributed data to the research community.  In the process of testing this approach, we hope to build scalable solutions for data sharing and network interaction between affordable and expensive sensors.  We are also engaging the citizen science and research communities with a new perspective on how to approach scalable, affordable sensing.   



%something to look into
%http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.714.3011&rep=rep1&type=pdf