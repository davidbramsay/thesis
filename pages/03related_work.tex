\chapter{3. Related Work}

\section{Air Quality}

\subsection{Safecast}

We are already closely linked with the Safecast team, which has been working on open data and open hardware air quality sensing for about a year.  They've seen a lot of success and clout with their radiation data, and have just finished an alpha version of a stationary air quality monitor.

\subsection{Aclima/Google}

Google and Aclima have started a mobile, car mounted air quality project.  However, their algorithms and hardware have been kept under wraps, and they likely intend to control the dataset.  The speculation is that they are using a quite expensive setup in the back of the car, slowly drawing in air over time.  Aclima has a reputation for litigious behavior, and it is generally understood that accurate mobile particulate sensing with a car-mounted system is still elusive.   

\subsection{Copenhagen Wheel Project}

Out of the MIT SENSEable lab, it is mounted on a bike wheel and can help assist your pedaling, lock your bike, and capture data about air quality.  The original project was unveiled in 2009, but it has since been licensed to a Cambridge Area startup (Superpedestrian) and is available for pre-order.  Air quality is a tertiary feature of their project, and no hardware specs or details about what they are planning to monitor have been released.

\subsection{OpenSense} 

An ETH based project, is the current world leader in mobile air quality sensor networks, with the first practical distributed mobile sensing platform using large devices mounted on the Zurich public tram system. The openSense team has published many excellent articles that address some of the fundamental problems in mobile air quality sensing and prediction with low cost sensors.  They've laid the groundwork for dealing with low cost gas sensor calibration (particularly with multi-hop calibration techniques), as well as advancing the state of the art in Land Use Regression modeling based on sparse, real-time, mobile, distributed measurement.  However, their resolution has only been tested up to 12 hour, 100m x 100m predictions (with accuracy much less than weekly/yearly predictions), and their sensors are large and run off of power from the trams they are mounted on. Their first attempt at truly personal sensing was a recent low-cost phone-enabled ozone sensor, which proved viabile.  However, some of their assumptions with this device included (1) ozone has limited and predictable spatiotemporal variation (which is true, much less than other pollutants), and (2) their preliminary tests on the effects of airflow revealed significant errors for high pollution levels.
	
\subsection{Cheap Sensor Development and Testing Groups}

There are organizations (such as SCAQMD and the EPA) that are testing and characterizing cheap optical PM sensors in real-world scenarios. There is also an active research community around developing MEMS PM sensors (using tech like FBAR).  These groups are essential for verifying the claims of new products, and have great insight into the trends of success and failure with new sensor technology.

\subsection{Other}

Other projects worth mentioning are MIT's stationary/expensive clarity sensing project, the independent and nicely designed tricorder sensor platform, the innovative propeller health project that extrapolates pollution data from asthmatic inhaler usage, the open source and mobile UPOD project, the mobile Italian uSense project, the 'wearable' AirBeam kickstarter project, the Boston based startup Elm that translates their distributed air quality data into useable advice (and has an open invitation on their site to hook your sensor into their network), the Israeli BreezoMeter startup that extrapolates street-level/high resolution air quality data (presumably from wind patterns and known reference data),  the London-based company cleanSpace that provides air quality maps and incentivizes air quality friendly commuting, the startups Clarity and Tzoa that are selling versions of the 'world's smallest' PM2.5 wearable, and the startup uHoo which is building a home air quality monitor.   Other slightly less interesting, but still notable, include a handful of other hardware/commercial projects (the Air Quality Egg, CMU's Speck, Atmotube, AirboxLab, Airmon, SmartCitizen) and independent research like the French citizenAir project and DIY Public Labs work.  New projects in this space appear on a regular basis. 

\section{Data Sharing Solutions}

There are many options for data-sharing, particularly when exploring options that have a centralized entrypoint.  Open source tools like Django allow system administrators to easily spin up their own database solutions and create front-end tools, while services like plenar.io offer simple, managed solutions for data upload, display, and download.  

An interesting open-source solution is Apache Hadoop, which is a database backend that can handle distributed storage and distributed processing for very large datasets.  It can run on Amazon Web Services or Google Cloud, and has been used by many large companies.  While interesting, it is not designed necessarily for easy input/output of data, but instead for managing and parallelizing computation on large datasets using Java (where everything is done in the cloud).  This solution is likely over-technical and over-specified for the air quality use case.  


\subsection{The Semantic Web - RDF, HAL, and JSON-LD}

An alternative, simple, distributed solution comes in the form of the semantic web.  Instead of thinking of our data as seperate from our devices-- where researchers must pull data by hand, process it, and then upload it to a repository-- it makes more sense to take an 'Internet of Things' approach.  The advantages of this system are many-- it provides transparency to the measurement technique (data is associated with the device, and metadata about the device, that uploaded it) as well as to the processing steps (data is uploaded in a raw form, and processed in the web).  It also provides an intuitive, physical heirarchy for organizing and linking device and sensor data resources together.  With proper system design, the researcher should not have to manually upload anything-- they should be able to automatically pull all of the latest data (including automatically calibrated/processed data) from the database without ever manually pushing it there in the first place.

This type of sensor network design is not new.  Most large sensor network installations automatically push their data up to a central server.  By adding a semantic web layer on top of these servers, it is simple to connect devices and sensors in a much larger 'Web of Things'.  Many standards for achieving this have been proposed over recent years.    

Resource Description Format (RDF) is one of the most noteworthy foundations in semantic web thinking.  It represents a data model specification-- every web resource (for instance, a device or a sensor object stored at a given URI)-- has defined relationships through 'triples' of the form subject-predicate-object.  An example would be 'device #1 includes sensor #5', which would be represented with URIs (http://device/1, http://relation/includes, http://sensor/5).  This creates a labeled, directed multi-graph data model.  It has been adapted to many standards and syntaxes like XML, and extended in standards like OWL (Web Ontology Language) or Turtle (Terse RDF Triple Language).  The syntactic extension of this format in JSON -- the lingua franca of web data-- is known as JSON-LD, and has been advocated for by Tim Berners-Lee (inventor of the World Wide Web). 

HAL, or Hypermedia Application Language, is another hypermedia semantic web data model with XML and JSON formats.  It similarly defines relations to other resources using a shared URI, so ontologies can be easily added and extended.  It is simpler than JSON-LD, with each resource limited to its attributes and its external link relations.  HAL supports simple embedded resources and URI prefixing.


\subsection{IoTivity and AllJoyn}

Industry consortia are starting to take steps to build their own 'Web of Things' using the internet backbone.  The two largest are IoTivity-- a project by the Linux Foundation and Samsung-- and Qualcomm's AllJoyn.  They support low level protocols like CoAP, large APIs for interfacing with connected devices, and end-to-end device discovery and connection solutions.  While these tools may gain prominence over the next several years, for now they are in their infancy, and looking to address more fundamental infrastructure/connectivity problems (focusing more on real-time queries and connectivity rather than data storage, management, and sharing).
		
\subsection{ChainAPI and TidMarsh}

ChainAPI is a standard for data sharing based on HAL and JSON.  It does not attempt to specify end-to-end connectivity, nor does it force any specific implementation or ontology-- it simply wraps the HAL semantic web specification with common sense design principles to make interoperability and data access simple and intuitive.  This level of specificity enables a very low barrier to entry and very flexible use, while still providing enough structure to make a large, coherent ecosystem realistic.

ChainAPI has been tested and used in the MIT Media Lab's TidMarsh project to store and expose ecological data from hundreds of local sensors over dozens of devices.  This implementation includes a browsable web front-end, and several forward looking applications that take advantage of ChainAPI's streamability-- a live, constantly updating virtual representation of the sensed environment is available at tidmarsh.media.mit.edu.  Electronic music compositions have been written that stream and use live data from the Marsh.  Several nice data visualization scripts have also been written on top of ChainAPI.

ChainAPI has a low barrier to entry, an extensible ontology, provides easy access to streamable data, and can quickly scale in a distributed manner.  It is as simple as possible without sacraficing functionality.