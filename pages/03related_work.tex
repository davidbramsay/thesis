\chapter{3. Related Work}

In this section we consider past air quality work in the context of mobile or personal sensing.  Additionally, we explore several of the cutting edge solutions for storing, manipulating, and displaying data of this type.

\section{Air Quality}

\subsection{Safecast}

Safecast \cite{safecast} is an organization created in response to the Fukushima disaster to crowd-source radiation measurements using open-source GPS-tracked geiger counters.  They have succeeded at creating and publishing the most detailed radiation maps in the world, and are now looking to apply their expertise to air quality.

We are already closely linked with the Safecast team, which has been working on open data and open hardware air quality sensing for about a year.  They've just finished an alpha version of a stationary air quality monitor.

\subsection{Aclima/Google}

Aclima \cite{aclima} is a San Francisco startup specializing in environmental sensor networks.  Last year, Google and Aclima started a mobile, car mounted air quality project.  However, their algorithms and hardware have not been shared and they appear to want to control all the data they generate.  The speculation is that they are using a quite expensive setup in the back of the car, slowly drawing in air over time.  

\subsection{Copenhagen Wheel Project}

Out of the MIT SENSEable lab, the Copenhagen wheel \cite{wheel} is mounted on a bike wheel to assist your pedaling, lock your bike, and capture data about air quality.  The original project was unveiled in 2009 and subsequently licensed to a Cambridge Area startup (Superpedestrian).  It is currently available for pre-order.  Air quality is a tertiary feature of their project, and no hardware specs or details about what they are planning to monitor have been released.

\subsection{OpenSense} 

An ETH based project, is the current world leader in mobile air quality sensor networks, with the first practical distributed mobile sensing platform using large devices mounted on the Zurich public tram system. \cite{hasen2011}  The OpenSense team has published many excellent articles that address some of the fundamental problems in low-cost, mobile air quality sensing.  They've laid the groundwork for dealing with gas sensor calibration (particularly with multi-hop calibration techniques), as well as advancing the state of the art in Land Use Regression modeling based on sparse, real-time, mobile, distributed measurement.  However, their resolution has only been tested up to 12 hour, 100m x 100m predictions (with accuracy much less than weekly/yearly predictions), and their sensors are large and run off of power from the trams they are mounted on. Their first attempt at truly personal sensing was a recent low-cost phone-enabled ozone sensor, which proved viable. \cite{hasen2012_2}  However, one of their assumptions for this device's success was that ozone has limited and predictable spatiotemporal variation (which is true, much less than other pollutants). Their preliminary tests with this sensor also revealed significant errors for high pollution levels when under airflow. \cite{hasen2014, hasen2012, saukh2015}
	
\subsection{TRUSS}

Similar to OpenSense, the TRUSS (Tracking Risk with Ubiquitous Smart Sensing) project from the Media Lab's Responsive Environments group similarly blends high-power, fixed sensors with low-power, wearable nodes to provide real-time sensor fusion and insight. \cite{truss}  In particular, this project examined and compared multiple air quality sensors to predict harmful conditions-- however, the goal of this work was to detect coarse, major risk instead of precise, ambient exposure.

\subsection{Cheap Sensor Development and Testing Groups}

There are organizations (such as South Coast Air Quality Management District [SCAQMD], the UK's National Physical Laboratory, and the EPA) that are testing and characterizing cheap optical PM sensors in real-world scenarios. There is also an active research community around developing MEMS PM sensors (using tech like FBAR, or Film Bulk Acoustic Resonators).  These groups are essential for independently testing the claims of new products, and provide important insight into the trends with respect to the success and failure of new sensor technology.

\subsection{Other}

There are many other projects that appear in the air quality space, each with some unique take on affordable, distributed air quality.  Most of the following projects are successful in one or more aspects of their system design, but do not innovate with their core sensor physics or algorithm. 

These projects include MIT's stationary/expensive Clarity sensing project, \cite{clarity} the independent and nicely designed tricorder sensor platform, \cite{tricorder} the innovative Propeller Health project that extrapolates pollution data from asthmatic inhaler usage, \cite{propeller} the open source and mobile UPOD project, \cite{upod} the mobile Italian uSense project, \cite{usense} the 'wearable' AirBeam kickstarter project, \cite{airbeam} the Boston based startup Elm that translates their distributed air quality data into useable advice (and has an open invitation on their site to hook your sensor into their network), \cite{elm} the Israeli BreezoMeter startup that extrapolates street-level/high resolution air quality data (presumably from wind patterns and known reference data), \cite{breeze} the London-based company cleanSpace that provides air quality maps and incentivizes air quality friendly commuting, \cite{cleanspace} the startups Clarity and Tzoa that are selling versions of the 'world's smallest' PM2.5 wearable, \cite{ clarity2, tzoa} and the startup uHoo which is building a home air quality monitor. \cite{uhoo}  Other slightly less interesting, but still notable, include a handful of other hardware/commercial projects (the Air Quality Egg, \cite{egg} CMU's Speck, \cite{speck} Atmotube, \cite{atmotube} AirboxLab, \cite{airbox} SmartCitizen \cite{sck}) and independent research like the French citizenAir project \cite{citizenair} and DIY Public Labs work. \cite{publiclabs}  New projects in this space appear on a regular basis. 

\section{Data Sharing Solutions}

There are many options for data-sharing, particularly when exploring options that have a centralized entrypoint.  Open source tools like Django \cite{django} allow system administrators to easily spin up their own database solutions and create front-end tools, while services like plenar.io offer simple, managed solutions for data upload, display, and download.  

An interesting open-source solution is Apache Hadoop, \cite{hadoop} which is a database backend that can handle distributed storage and distributed processing for very large datasets.  It can run on Amazon Web Services or Google Cloud, and has been used by many large companies.  While interesting, it is not designed for easy input/output of data, but instead for managing and parallelizing computation on large datasets using Java (where everything is done in the cloud).  This solution is likely over-technical and over-specified for the air quality use case.  


\subsection{The Semantic Web - RDF, HAL, and JSON-LD}

An alternative, simple, distributed solution comes in the form of the semantic web.  Instead of thinking of our data as separate from our devices-- where researchers must pull data by hand, process it, and then upload it to a repository-- it makes more sense to take an 'Internet of Things' approach.  The advantages of this system are many-- it provides transparency to the measurement technique (data is associated with the device, and metadata about the device, that uploaded it) as well as to the processing steps (data is uploaded in a raw form, and processed in the web).  It also provides an intuitive, physical hierarchy for organizing and linking device and sensor data resources together.  With proper system design, the researcher should not have to manually upload anything-- they should be able to automatically pull all of the latest data (including automatically calibrated/processed data) from the database without ever manually pushing it there in the first place.

This type of sensor network design is not new.  Most large sensor network installations automatically push their data up to a central server.  By adding a semantic web layer on top of these servers, it is simple to connect devices and sensors in a much larger 'Web of Things'.  Many standards for achieving this have been proposed over recent years.    

Resource Description Format (RDF) is one of the most noteworthy foundations in semantic web thinking. \cite{RDF}  It represents a data model specification-- every web resource (for instance, a device or a sensor object stored at a given URI)-- has defined relationships through 'triples' of the form subject-predicate-object.  An example would be 'device #1 includes sensor #5', which would be represented with URIs (http://device/1, http://relation/includes, http://sensor/5).  This creates a labeled, directed multi-graph data model.  It has been adapted to many standards and syntaxes like XML, and extended in standards like OWL (Web Ontology Language) or Turtle (Terse RDF Triple Language).  The syntactic extension of this format in JSON -- the current lingua franca of web data-- is known as JSON-LD, and has been advocated for by Tim Berners-Lee (inventor of the World Wide Web). \cite{jsonld}

HAL, or Hypertext Application Language, is another hypermedia semantic web data model with XML and JSON formats. \cite{hal}  It similarly defines relations to other resources using a shared URI, so ontologies can be easily added and extended.  It is simpler than JSON-LD, with each resource limited to its attributes and its external link relations.  HAL supports simple embedded resources and URI prefixing.


\subsection{IoTivity and AllJoyn}

Industry consortia are starting to take steps to build their own 'Web of Things' using the internet backbone.  The two largest are IoTivity-- a project by the Linux Foundation and Samsung-- and Qualcomm's AllJoyn. \cite{iot, alljoyn}  They support low level protocols like CoAP, large APIs for interfacing with connected devices, and end-to-end device discovery and connection solutions.  While these tools may gain prominence over the next several years, for now they are in their infancy, and looking to address more fundamental infrastructure/connectivity problems (focusing more on real-time queries and connectivity rather than data storage, management, and sharing).
		
\subsection{ChainAPI and TidMarsh}

ChainAPI \cite{chainGit, chainPaper} is a standard for data sharing based on HAL and JSON developed in the Responsive Environments group at the MIT Media Lab.  It does not attempt to specify end-to-end connectivity, nor does it force any specific implementation or ontology-- it simply wraps the HAL semantic web specification with common-sense design principles to make interoperability and data access simple and intuitive.  This level of specificity enables a very low barrier to entry and very flexible use, while still providing enough structure to support a large, coherent ecosystem.

ChainAPI has been tested and used in the MIT Media Lab's TidMarsh project to store and expose ecological data from hundreds of local sensors over dozens of devices.  This implementation includes a browsable web front-end, and several forward looking applications that take advantage of ChainAPI's streamability-- a live, constantly updating virtual representation of the sensed environment is available at tidmarsh.media.mit.edu.  Electronic music compositions have been written that stream and use live data from the Marsh. \cite{evan}  Several elaborate data visualization scripts have also been written on top of ChainAPI. \cite{tidviz}

ChainAPI has a low barrier to entry, an extensible ontology, provides easy access to streamable data, and can quickly scale in a distributed manner.  It is as simple as possible without sacrificing functionality.